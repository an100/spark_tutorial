{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Creating an RDD</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating the 3 RDDs from the different datasets from Amazon product reviews. Note that it does not move the data at this stage due to the lazy evaluation nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1113cb1d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data using the Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fashion = sc.textFile('Data/Reviews/fashion.json')\n",
    "electronics = sc.textFile('Data/Reviews/electronics.json')\n",
    "sports = sc.textFile('Data/Reviews/sports.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has happened, why is that?\n",
    "In Spark, some operations are *transformations*, which are lazily evaluated and others are *actions*.\n",
    "\n",
    "Read more here: http://spark.apache.org/docs/latest/programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  PythonRDD[6] at RDD at PythonRDD.scala:43\n",
      "Result 2:  10000\n"
     ]
    }
   ],
   "source": [
    "# Example of a basic transformation\n",
    "print \"Result 1: \", fashion.map(lambda x: len(x))\n",
    "\n",
    "# Example of an action:\n",
    "print \"Result 2: \", fashion.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some basic data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion has 10000 rows, electronics 10000 rows and sports 10000 rows\n",
      "\n",
      "fashion first row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'{\"reviewerID\": \"A2XVJBSRI3SWDI\", \"asin\": \"0000031887\", \"reviewerName\": \"abigail\", \"helpful\": [0, 0], \"reviewText\": \"Perfect red tutu for the price. I baught it as part of my daughters Halloween costume and it looked great on her.\", \"overall\": 5.0, \"summary\": \"Nice tutu\", \"unixReviewTime\": 1383523200, \"reviewTime\": \"11 4, 2013\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"fashion has {0} rows, electronics {1} rows and sports {2} rows\\n\".format(fashion.count(), electronics.count(), sports.count())\n",
    "print \"fashion first row:\"\n",
    "fashion.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can union them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"reviewerID\": \"A2XVJBSRI3SWDI\", \"asin\": \"0000031887\", \"reviewerName\": \"abigail\", \"helpful\": [0, 0], \"reviewText\": \"Perfect red tutu for the price. I baught it as part of my daughters Halloween costume and it looked great on her.\", \"overall\": 5.0, \"summary\": \"Nice tutu\", \"unixReviewTime\": 1383523200, \"reviewTime\": \"11 4, 2013\"}\n"
     ]
    }
   ],
   "source": [
    "union_of_rdds = fashion.union(electronics).union(sports)\n",
    "print union_of_rdds.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now parse the file using the json library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'asin': u'0000031887',\n",
       " u'helpful': [0, 0],\n",
       " u'overall': 5.0,\n",
       " u'reviewText': u'Perfect red tutu for the price. I baught it as part of my daughters Halloween costume and it looked great on her.',\n",
       " u'reviewTime': u'11 4, 2013',\n",
       " u'reviewerID': u'A2XVJBSRI3SWDI',\n",
       " u'reviewerName': u'abigail',\n",
       " u'summary': u'Nice tutu',\n",
       " u'unixReviewTime': 1383523200}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "parsed_fashion = fashion.map(lambda x: json.loads(x))\n",
    "parsed_fashion.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of loading files is by using a list of comma-separated file paths or a wildcard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile('Data/Reviews/fashion.json,Data/Reviews/electronics.json,Data/Reviews/sports.json').map(lambda x: json.loads(x))\n",
    "\n",
    "# QUESTION: How many partitions does the rdd have?\n",
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine we want to know the number of lines in each partition. For that, we need to access the data in each single partition and run operations on them instead of on each row.\n",
    "\n",
    "For this, we will use mapPartitionsWithIndex which takes a partition index and an iterator over the data as arguments. Each function in the API is documented in: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition 0 has 10000 rows\n",
      "partition 1 has 10000 rows\n",
      "partition 2 has 10000 rows\n"
     ]
    }
   ],
   "source": [
    "indexed_data = data.mapPartitionsWithIndex(lambda splitIndex, it: [(splitIndex, len([x for x in it]))])\n",
    "\n",
    "for num_partition, count_partition in indexed_data.collect():\n",
    "    print \"partition {0} has {1} rows\".format(num_partition, count_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Reducers</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we have been tasked to do is **to get the minimum and maximum number of reviews per product**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of reviews is 2033, min number of reviews is 1\n"
     ]
    }
   ],
   "source": [
    "product_num = data.map(lambda x: (x['asin'], 1)).reduceByKey(lambda x,y: x+y)\n",
    "# The rdd product_num will contain (product_asin, total_number_reviews)\n",
    "\n",
    "# What are the maximum and minimum number of reviews?\n",
    "max_num = product_num.map(lambda x: x[1]).max()\n",
    "min_num = product_num.map(lambda x: x[1]).min()\n",
    "\n",
    "print \"Max number of reviews is {0}, min number of reviews is {1}\".format(max_num, min_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Images/reducebykey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: what is the max score for each product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Joining multiple sources</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to join the product reviews by users to the product metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'asin': u'0000037214', u'title': u'Purple Sequin Tiny Dancer Tutu Ballet Dance Fairy Princess Costume Accessory', u'price': 6.99, u'imUrl': u'http://ecx.images-amazon.com/images/I/31mCncNuAZL.jpg', u'related': {u'also_viewed': [u'B00JO8II76', u'B00DGN4R1Q', u'B00E1YRI4C']}, u'salesRank': {u'Clothing': 1233557}, u'brand': u'Big Dreams', u'categories': [[u'Clothing, Shoes & Jewelry', u'Girls'], [u'Clothing, Shoes & Jewelry', u'Novelty, Costumes & More', u'Costumes & Accessories', u'More Accessories', u'Kids & Baby']]}\n"
     ]
    }
   ],
   "source": [
    "product_metadata = sc.textFile('Data/Products/sample_metadata.json').map(lambda x: json.loads(x))\n",
    "print product_metadata.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten_categories(line):\n",
    "    old_cats = line['categories']\n",
    "    line['categories'] = [item for sublist in old_cats for item in sublist]\n",
    "    return line\n",
    "\n",
    "product_metadata = product_metadata.map(flatten_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to join the review data to the metadata about the product. We can use the ASIN for that, which is a unique identifier for each product. In order to do a join, we need to turn each structure into key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are joining 30000 product reviews to 2469 rows of metadata information about the products.\n",
      "\n",
      "First row of key_val_data:\n",
      "(u'0000031887', {u'reviewerID': u'A2XVJBSRI3SWDI', u'asin': u'0000031887', u'reviewerName': u'abigail', u'helpful': [0, 0], u'reviewText': u'Perfect red tutu for the price. I baught it as part of my daughters Halloween costume and it looked great on her.', u'overall': 5.0, u'summary': u'Nice tutu', u'unixReviewTime': 1383523200, u'reviewTime': u'11 4, 2013'})\n"
     ]
    }
   ],
   "source": [
    "key_val_data = data.map(lambda x: (x['asin'], x))\n",
    "key_val_metadata = product_metadata.map(lambda x: (x['asin'], x))\n",
    "\n",
    "print \"We are joining {0} product reviews to {1} rows of metadata information about the products.\\n\".format(key_val_data.count(),key_val_metadata.count())\n",
    "print \"First row of key_val_data:\"\n",
    "print key_val_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number partitions key_val_data:  3\n",
      "number partitions key_val_metadata:  2\n",
      "\n",
      "For key 8179050874:\n",
      "\n",
      "the review is {u'reviewerID': u'A1IQJSHCMW69O5', u'asin': u'8179050874', u'reviewerName': u'Jose Perez', u'helpful': [0, 0], u'reviewText': u\"I bought this item because of the description that is for the Blackberry bold, to my surprise is for the curve it doesn't fit the screen there is like one inch of screen not protected by the screen, also it reflects sunlight making the screen virtually unusable when outdoors, and looks ugly..\", u'overall': 1.0, u'summary': u'This is not for Bold is for Curve', u'unixReviewTime': 1242518400, u'reviewTime': u'05 17, 2009'}\n",
      "\n",
      "the product metadata is {u'asin': u'8179050874', u'salesRank': {u'Electronics': 324466}, u'imUrl': u'http://ecx.images-amazon.com/images/I/41f2QHnWYNL._SY300_.jpg', u'categories': [u'Electronics', u'Computers & Accessories', u'Laptop & Netbook Computer Accessories', u'Batteries'], u'title': u'PRIVACY Screen Saver for your BLACKBERRY Bold 9000 ! Shield and Prevent others from viewing your information while protecting your phone!'}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"number partitions key_val_data: \", \n",
    "print key_val_data.getNumPartitions()\n",
    "print \"number partitions key_val_metadata: \", \n",
    "print key_val_metadata.getNumPartitions()\n",
    "print\n",
    "\n",
    "joined = key_val_data.join(key_val_metadata)\n",
    "\n",
    "key, (review, product) = joined.first()\n",
    "print \"For key {0}:\\n\\nthe review is {1}\\n\\nthe product metadata is {2}.\\n\".format(key, review, product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of output partitions of the join? To understand this, the best is to refer back to the Pyspark source code: https://github.com/apache/spark/blob/branch-1.3/python/pyspark/join.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 partitions\n"
     ]
    }
   ],
   "source": [
    "# QUESTION: what is the number of partitions of the joined dataset?\n",
    "\n",
    "print \"There are {0} partitions\".format(joined.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to manipulate, we will change the structure of the joined rdd to be a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0:\n",
      "\n",
      "{u'reviewerID': u'A1IQJSHCMW69O5', u'asin': u'8179050874', u'reviewerName': u'Jose Perez', u'helpful': [0, 0], u'title': u'PRIVACY Screen Saver for your BLACKBERRY Bold 9000 ! Shield and Prevent others from viewing your information while protecting your phone!', u'imUrl': u'http://ecx.images-amazon.com/images/I/41f2QHnWYNL._SY300_.jpg', u'reviewText': u\"I bought this item because of the description that is for the Blackberry bold, to my surprise is for the curve it doesn't fit the screen there is like one inch of screen not protected by the screen, also it reflects sunlight making the screen virtually unusable when outdoors, and looks ugly..\", u'overall': 1.0, u'summary': u'This is not for Bold is for Curve', u'unixReviewTime': 1242518400, u'salesRank': {u'Electronics': 324466}, u'reviewTime': u'05 17, 2009', u'categories': [u'Electronics', u'Computers & Accessories', u'Laptop & Netbook Computer Accessories', u'Batteries']}\n",
      "\n",
      "row 1:\n",
      "\n",
      "{u'reviewerID': u'A2HC8YQVZ4HMF5', u'asin': u'8179050874', u'reviewerName': u'Wowbagger the Infinitely Prolonged', u'helpful': [0, 0], u'title': u'PRIVACY Screen Saver for your BLACKBERRY Bold 9000 ! Shield and Prevent others from viewing your information while protecting your phone!', u'imUrl': u'http://ecx.images-amazon.com/images/I/41f2QHnWYNL._SY300_.jpg', u'reviewText': u'Despite being sold specifically for the Blackberry Bold 9000, it simply doesn\\'t fit a Blackberry Bold.The screen protector is about a third of a millimetre too wide. As a result, the chrome trim around the outside of the Blackberry prevents it from lying flat on the edges of the screen so it does not attach to the screen properly: there is always a 2-3 millimetres of \"air margin\" down either one or both sides.The problems are therefore:1.  It looks ugly2.  It will fill with dust3.  Case-mate support have been messing me around for over a month now and I\\'m beginning to suspect they are just hoping that I\\'ll go away and stop annoying them.  In other words, the tech support is as useless as the product...', u'overall': 1.0, u'summary': u\"Doesn't even fit the screen...\", u'unixReviewTime': 1238025600, u'salesRank': {u'Electronics': 324466}, u'reviewTime': u'03 26, 2009', u'categories': [u'Electronics', u'Computers & Accessories', u'Laptop & Netbook Computer Accessories', u'Batteries']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_dictionaries(metadata_line, review_line):\n",
    "    new_dict = review_line\n",
    "    new_dict.update(metadata_line)\n",
    "    return new_dict\n",
    "\n",
    "nice_joined = joined.map(lambda x: merge_dictionaries(x[1][0], x[1][1]))\n",
    "row0, row1 = nice_joined.take(2)\n",
    "\n",
    "print \"row 0:\\n\\n{0}\\n\\nrow 1:\\n\\n{1}\\n\".format(row0, row1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. GroupByKey</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have joined two data sources, we can start doing some ad-hoc analysis of the data! Now the task is **to get the average product review length for each category**. The categories are encoded as a list of categories, so we first need to 'flatten them out'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice_joined.cache()\n",
    "nice_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_categories.take(5):\n",
      "\n",
      "[u'Electronics', u'Computers & Accessories', u'Laptop & Netbook Computer Accessories', u'Batteries']\n",
      "[u'Electronics', u'Computers & Accessories', u'Laptop & Netbook Computer Accessories', u'Batteries']\n",
      "[u'Clothing, Shoes & Jewelry', u'Novelty, Costumes & More', u'Costumes & Accessories', u'Costumes', u'Kids & Baby', u'Infants & Toddlers', u'Baby Boys']\n",
      "[u'Clothing, Shoes & Jewelry', u'Novelty, Costumes & More', u'Costumes & Accessories', u'Costumes', u'Kids & Baby', u'Infants & Toddlers', u'Baby Boys']\n",
      "[u'Sports & Outdoors', u'Outdoor Gear', u'Camping & Hiking', u'Camp Bedding', u'Sleeping Pads', u'Foam Pads']\n",
      "\n",
      "flat_categories.take(5):\n",
      "\n",
      "Electronics\n",
      "Computers & Accessories\n",
      "Laptop & Netbook Computer Accessories\n",
      "Batteries\n",
      "Electronics\n",
      "\n",
      "There are 925 categories.\n"
     ]
    }
   ],
   "source": [
    "original_categories = nice_joined.map(lambda x: x['categories'])\n",
    "flat_categories = nice_joined.flatMap(lambda x: x['categories'])\n",
    "\n",
    "print \"original_categories.take(5):\\n\"\n",
    "print '\\n'.join([str(x) for x in original_categories.take(5)]) + '\\n'\n",
    "\n",
    "print \"flat_categories.take(5):\\n\"\n",
    "print '\\n'.join([str(x) for x in flat_categories.take(5)]) + '\\n'\n",
    "\n",
    "num_categories = flat_categories.distinct().count()\n",
    "print \"There are {0} distinct categories.\".format(num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to get the average review length across all categories, we will use a new function: groupByKey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the flatMap: (u'Electronics', 293)\n",
      "After the groupByKey: (u'Screen Protectors', [191, 208, 135, 163, 108, 782, 161, 1002, 446, 85])\n",
      "\n",
      "grouped_category_review.first(): (u'Screen Protectors', 328.1)\n",
      "\n",
      "The top 10 categories are: [(u'Photos', 4305.5), (u'Motets', 3404.0), (u'Free-Weight Racks', 3404.0), (u'Weight Racks', 3404.0), (u'Magnificats', 3404.0), (u'Bags, Packs & Accessories', 3281.5), (u'Rugby', 3156.0), (u'Rifles', 2066.6666666666665), (u'Soul-Jazz & Boogaloo', 1945.0), (u'Sonatinas', 1908.8)]\n"
     ]
    }
   ],
   "source": [
    "category_review = nice_joined.flatMap(lambda x: [(y, len(x['reviewText'])) for y in x['categories']])\n",
    "print \"After the flatMap: \" + str(category_review.first())\n",
    "print \"After the groupByKey: \" + str(category_review.groupByKey().map(lambda x: (x[0], list(x[1]))).first())\n",
    "print\n",
    "\n",
    "grouped_category_review = category_review.groupByKey().map(lambda x: (x[0], sum(x[1])/float(len(x[1]))))\n",
    "print \"grouped_category_review.first(): \" + str(grouped_category_review.first()) + '\\n'\n",
    "\n",
    "### Now we can sort the categories by average product review length\n",
    "print \"The top 10 categories are: \" + str(sorted(grouped_category_review.collect(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Do the same thing, but this time you are not allowed to use groupByKey()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional: Data skewness</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Creating the large rdd***\n",
      "first 5 items:[(0, 0), (1, 0), (1, 1), (2, 0), (2, 1)]\n",
      "num rows: 5171502\n",
      "num partitions: 16\n",
      "The distribution of elements per partition is [(0, 1), (1, 2), (2, 7), (3, 20), (4, 54), (5, 148), (6, 403), (7, 1096), (8, 2980), (9, 8103), (10, 22026), (11, 59874), (12, 162754), (13, 442413), (14, 1202604), (15, 3269017)]\n",
      "\n",
      "***Creating the small rdd***\n",
      "first 5 items:[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n",
      "num rows: 16\n",
      "num partitions: 16\n",
      "The distribution of elements per partition is [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n",
      "\n",
      "Joining them\n",
      "The direct join takes 0:00:37.777983\n",
      "The joined rdd has 32 partitions and 5171502 rows\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from datetime import datetime\n",
    "\n",
    "def get_part_index(splitIndex, iterator):\n",
    "    for it in iterator:\n",
    "        yield (splitIndex, it)\n",
    "       \n",
    "def count_elements(splitIndex, iterator):\n",
    "    n = sum(1 for _ in iterator)\n",
    "    yield (splitIndex, n)\n",
    "        \n",
    "print \"***Creating the large rdd***\"\n",
    "num_parts = 16\n",
    "# create the large skewed rdd\n",
    "skewed_large_rdd = sc.parallelize(range(0,num_parts), num_parts).flatMap(lambda x: range(0, int(exp(x)))).mapPartitionsWithIndex(lambda ind, x: get_part_index(ind, x)).cache()\n",
    "print \"first 5 items:\" + str(skewed_large_rdd.take(5))\n",
    "print \"num rows: \" + str(skewed_large_rdd.count())\n",
    "print \"num partitions: \" + str(skewed_large_rdd.getNumPartitions())\n",
    "print \"The distribution of elements per partition is \" + str(skewed_large_rdd.mapPartitionsWithIndex(lambda ind, x: count_elements(ind, x)).collect())\n",
    "print\n",
    "\n",
    "print \"***Creating the small rdd***\"\n",
    "small_rdd = sc.parallelize(range(0,num_parts), num_parts).map(lambda x: (x, x))\n",
    "print \"first 5 items:\" + str(small_rdd.take(5))\n",
    "print \"num rows: \" + str(small_rdd.count())\n",
    "print \"num partitions: \" + str(small_rdd.getNumPartitions())\n",
    "print \"The distribution of elements per partition is \" + str(small_rdd.mapPartitionsWithIndex(lambda ind, x: count_elements(ind, x)).collect())\n",
    "\n",
    "print\n",
    "\n",
    "print \"Joining them\"\n",
    "t0 = datetime.now()\n",
    "result = skewed_large_rdd.leftOuterJoin(small_rdd)\n",
    "result.count() \n",
    "print \"The direct join takes %s\"%(str(datetime.now() - t0))\n",
    "print \"The joined rdd has {0} partitions and {1} rows\".format(result.getNumPartitions(), result.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional: Integrating Spark with popular Python libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Perfect red tutu for the price. I baught it as part of my daughters Halloween costume and it looked great on her.',\n",
       " 'fashion')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "model = pickle.load(open('Data/classifiers/classifier.pkl', 'r'))\n",
    "model\n",
    "bla = fashion.map(lambda x: eval(x)['reviewText']).first()\n",
    "model_b = sc.broadcast(model)\n",
    "fashion.map(lambda x: eval(x)['reviewText']).map(lambda x: (x, model_b.value.predict([x])[0])).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Part 2: Spark DataFrame API and Spark SQL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "\n",
    "This is the latter part of the tutorial. The main focus will be on Spark DataFrames and Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews : 30000\n",
      "sample row : \n",
      "{\"reviewerID\": \"AKM1MP6P0OYPR\", \"asin\": \"0132793040\", \"reviewerName\": \"Vicki Gibson \\\"momo4\\\"\", \"helpful\": [1, 1], \"reviewText\": \"Corey Barker does a great job of explaining Blend Modes in this DVD. All of the Kelby training videos are great but pricey to buy individually. If you really want bang for your buck just subscribe to Kelby Training online.\", \"overall\": 5.0, \"summary\": \"Very thorough\", \"unixReviewTime\": 1365811200, \"reviewTime\": \"04 13, 2013\"}\n"
     ]
    }
   ],
   "source": [
    "review_filepaths = 'Data/Reviews/*'\n",
    "textRDD = sc.textFile(review_filepaths)\n",
    "\n",
    "print 'number of reviews : {0}'.format(textRDD.count())\n",
    "\n",
    "print 'sample row : \\n{0}'.format(textRDD.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Loading Data into a DataFrame</h1>\n",
    "\n",
    "A DataFrame requires schema. There are two main functions that can be used to assign schema into an RDD. \n",
    "+ Inferring Schema : This functions infers the schema of the RDD by observing it\n",
    "+ Applying Schema  : This function applies a manually defined schema an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1134d2390>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You need SQL context do \n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# # Instantiate SQL Context\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext\n",
    "# sqlContext\n",
    "\n",
    "# print sqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inferring the Schema Using Reflection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin=u'0132793040', helpful=[1, 1], overall=5.0, reviewText=u'Corey Barker does a great job of explaining Blend Modes in this DVD. All of the Kelby training videos are great but pricey to buy individually. If you really want bang for your buck just subscribe to Kelby Training online.', reviewTime=u'04 13, 2013', reviewerID=u'AKM1MP6P0OYPR', reviewerName=u'Vicki Gibson \"momo4\"', summary=u'Very thorough', unixReviewTime=1365811200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferredDF = sqlContext.read.json(review_filepaths)\n",
    "inferredDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferredDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Manually Specifying the Schema</h2>\n",
    "\n",
    "The Documentation about different data types can be found at [Spark SQL DataTypes section](https://spark.apache.org/docs/latest/sql-programming-guide.html#data-types \"Spark SQL DataTypes Documentation\") \n",
    "+ Defining the schema can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(reviewerID,StringType,true),StructField(asin,StringType,true),StructField(reviewerName,StringType,true),StructField(helpful,ArrayType(IntegerType,true),true),StructField(reviewText,StringType,true),StructField(reviewTime,StringType,true),StructField(overall,DoubleType,true),StructField(summary,StringType,true),StructField(unixReviewTime,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Export the modules\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define Schema\n",
    "REVIEWS_SCHEMA_DEF = StructType([\n",
    "        StructField('reviewerID', StringType(), True),\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('reviewerName', StringType(), True),\n",
    "        StructField('helpful', ArrayType(\n",
    "                IntegerType(), True), \n",
    "            True),\n",
    "        StructField('reviewText', StringType(), True),\n",
    "        StructField('reviewTime', StringType(), True),\n",
    "        StructField('overall', DoubleType(), True),\n",
    "        StructField('summary', StringType(), True),\n",
    "        StructField('unixReviewTime', LongType(), True)\n",
    "    ])\n",
    "\n",
    "print REVIEWS_SCHEMA_DEF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*QUESTION*: What do you think will happen if *QUESTION*: What do you think will happen if we remove some fields from this schema?\n",
    "\n",
    "1. The schema fails\n",
    "2. The schema works fine\n",
    "\n",
    "ANSWER???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(reviewerID=u'AKM1MP6P0OYPR', asin=u'0132793040', reviewerName=u'Vicki Gibson \"momo4\"', helpful=[1, 1], reviewText=u'Corey Barker does a great job of explaining Blend Modes in this DVD. All of the Kelby training videos are great but pricey to buy individually. If you really want bang for your buck just subscribe to Kelby Training online.', reviewTime=u'04 13, 2013', overall=5.0, summary=u'Very thorough', unixReviewTime=1365811200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a handcrafted schema with to create a DataFrame\n",
    "appliedDF = sqlContext.read.json(review_filepaths,schema=REVIEWS_SCHEMA_DEF)\n",
    "appliedDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. DataFrame operations</h1>\n",
    "\n",
    "Spark DataFrame API allow you to do multiple operations on the Data. The primary advantage of using the DataFrame API is that you can do data transoformations with the high level API without having to use Python. Using the high level API has its advantages which will be explained later in the tutorial.\n",
    "\n",
    "DataFrame API have functionality similar to that of Core RDD API. For example: \n",
    "+ map                     : foreach, Select\n",
    "+ mapPartition            : foreachPartition\n",
    "+ filter                  : filter\n",
    "+ groupByKey, reduceByKey : groupBy \n",
    "\n",
    "<h2>6.1. Selecting Columns</h2>\n",
    "\n",
    "You can use SELECT statement to select columns from your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'appliedDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de348c493c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m columnDF = appliedDF.select(appliedDF.asin,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0mappliedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0mappliedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mappliedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpful\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mappliedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpful\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mappliedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviewerID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'appliedDF' is not defined"
     ]
    }
   ],
   "source": [
    "columnDF = appliedDF.select(appliedDF.asin,\n",
    "                            appliedDF.overall,\n",
    "                            appliedDF.reviewText,\n",
    "                            appliedDF.helpful[0]/appliedDF.helpful[1],\n",
    "                            appliedDF.reviewerID,\n",
    "                            appliedDF.unixReviewTime).\\\n",
    "                    withColumnRenamed('(helpful[0] / helpful[1])','helpful')\n",
    "columnDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.2. Missing Values</h2>\n",
    "\n",
    "Similar to Pandas, DataFrames come equipped with functions to address missing data.\n",
    "+ dropna function: can be used to remove observations with missing values\n",
    "+ fillna function: can be used to fill missing values with a default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0321732944|    5.0|While many beginn...|               0.0|A2CX7LUOHB2NDG|    1341100800|\n",
      "|0439886341|    1.0|It never worked. ...|               1.0|A2NWSAGRHCP8N5|    1367193600|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0439886341|    1.0|Do not waste your...|               1.0|A1GI0U4ZRJA8WN|    1334707200|\n",
      "|0511189877|    5.0|Dog got the old r...|               0.0|A1QGNMC6O1VW39|    1397433600|\n",
      "|0511189877|    2.0|This remote, for ...|               1.0|A3J3BRHTDRFJ2G|    1397433600|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|\n",
      "|0511189877|    5.0|This unit works j...|               0.0|A34ATBPOK6HCHY|    1395532800|\n",
      "|0511189877|    5.0|It is an exact du...|               0.0| A89DO69P0XZ27|    1395446400|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|\n",
      "|0528881469|    5.0|Love it has every...|               0.0|A1DA3W4GTFXP6O|    1405641600|\n",
      "|0528881469|    1.0|I have owned two ...|               0.0|A29LPQQDG7LD5J|    1352073600|\n",
      "|0528881469|    5.0|We got this GPS f...|               0.0| AO94DHGC771SJ|    1370131200|\n",
      "|0528881469|    1.0|I'm a professiona...|               0.8| AMO214LNFCEI4|    1290643200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    2.0|Not going to writ...|               0.9|A1H8PY3QHMQQA0|    1290556800|\n",
      "|0528881469|    2.0|My brother is a t...|           0.71875| A2CPBQ5W4OGBX|    1277078400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get null observations out\n",
    "densedDF=columnDF.dropna(subset=[\"overall\"]).fillna(0.0,subset=[\"helpful\"]) \n",
    "densedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.3. Filtering rows</h2>\n",
    "\n",
    "Filtering lets you select rows based on arguments. The implementation pattern is similar to filtering RDDs, But simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|\n",
      "|0321732944|    5.0|While many beginn...|               0.0|A2CX7LUOHB2NDG|    1341100800|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|\n",
      "|0511189877|    5.0|Dog got the old r...|               0.0|A1QGNMC6O1VW39|    1397433600|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|\n",
      "|0511189877|    5.0|This unit works j...|               0.0|A34ATBPOK6HCHY|    1395532800|\n",
      "|0511189877|    5.0|It is an exact du...|               0.0| A89DO69P0XZ27|    1395446400|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|\n",
      "|0528881469|    5.0|Love it has every...|               0.0|A1DA3W4GTFXP6O|    1405641600|\n",
      "|0528881469|    5.0|We got this GPS f...|               0.0| AO94DHGC771SJ|    1370131200|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|\n",
      "|0528881469|    5.0|I did a lot of co...|               1.0|A37K02NKUIT68K|    1293235200|\n",
      "|0528881469|    4.0|I purchased this ...|               0.5|A2AW1SSVUIYV9Y|    1289001600|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|0.7142857142857143|A2AEHUKOV014BP|    1284249600|\n",
      "|0528881469|    4.0|Well as one of th...|               1.0|A2O8FIJR9EBU56|    1278547200|\n",
      "|0528881469|    4.0|Was fast and what...|               0.0| AYTBGUX49LF3W|    1398470400|\n",
      "|0528881469|    5.0|We had the GPS fo...|               0.0|A1E4WG8HRWWK4R|    1390867200|\n",
      "|0528881469|    5.0|Back in the old d...|               0.5|A2AOEW5UGXFOOQ|    1294790400|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF=densedDF.filter(densedDF.overall>=3)\n",
    "filteredDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.4. Grouping by overall scores</h2>\n",
    "\n",
    "Grouping is equivalent to the groupByKey in the core RDD API. You can transform the grouped values using a summary action such as:\n",
    "+ count\n",
    "+ sum\n",
    "+ average\n",
    "+ max and so on ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|overall|count|\n",
      "+-------+-----+\n",
      "|    3.0| 2128|\n",
      "|    5.0|18503|\n",
      "|    4.0| 5324|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped = filteredDF.groupBy(\"overall\").count()\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.5. Joining DataFrames together</h2>\n",
    "\n",
    "You can join two DataFrames together by using a common key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'{\"asin\": \"0000037214\", \"title\": \"Purple Sequin Tiny Dancer Tutu Ballet Dance Fairy Princess Costume Accessory\", \"price\": 6.9900000000000002, \"imUrl\": \"http://ecx.images-amazon.com/images/I/31mCncNuAZL.jpg\", \"related\": {\"also_viewed\": [\"B00JO8II76\", \"B00DGN4R1Q\", \"B00E1YRI4C\"]}, \"salesRank\": {\"Clothing\": 1233557}, \"brand\": \"Big Dreams\", \"categories\": [[\"Clothing, Shoes & Jewelry\", \"Girls\"], [\"Clothing, Shoes & Jewelry\", \"Novelty, Costumes & More\", \"Costumes & Accessories\", \"More Accessories\", \"Kids & Baby\"]]}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_filepaths = 'Data/Products/*'\n",
    "productRDD = sc.textFile(product_filepaths)\n",
    "productRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+\n",
      "|      asin|               title|price|          categories|\n",
      "+----------+--------------------+-----+--------------------+\n",
      "|0000037214|Purple Sequin Tin...| 6.99|[WrappedArray(Clo...|\n",
      "|0000032069|Adult Ballet Tutu...| 7.89|[WrappedArray(Spo...|\n",
      "|0000031909|Girls Ballet Tutu...|  7.0|[WrappedArray(Spo...|\n",
      "|0000032034|Adult Ballet Tutu...| 7.87|[WrappedArray(Spo...|\n",
      "|0000031852|Girls Ballet Tutu...| 3.17|[WrappedArray(Spo...|\n",
      "|0000032050|Adult Ballet Tutu...|12.85|[WrappedArray(Spo...|\n",
      "|0000031887|Ballet Dress-Up F...| 6.79|[WrappedArray(Clo...|\n",
      "|0000031895|Girls Ballet Tutu...| 2.99|[WrappedArray(Spo...|\n",
      "|0123456479|SHINING IMAGE HUG...|64.98|[WrappedArray(Clo...|\n",
      "|0132793040|Kelby Training DV...| null|[WrappedArray(Ele...|\n",
      "|0188477284|Klean Kanteen Cla...| null|[WrappedArray(Spo...|\n",
      "|0321732944|Kelby Training DV...| null|[WrappedArray(Ele...|\n",
      "|0439886341|Digital Organizer...| 8.15|[WrappedArray(Ele...|\n",
      "|0456844570|RiZ Women's Beaut...| null|[WrappedArray(Clo...|\n",
      "|0456808574|Lantin White Viso...| null|[WrappedArray(Clo...|\n",
      "|0456830197|NVC Unisex Light ...| null|[WrappedArray(Clo...|\n",
      "|0456856293|Kismeth Eyewear C...| null|[WrappedArray(Clo...|\n",
      "|0456840532|Max-MPH Black - L...| null|[WrappedArray(Clo...|\n",
      "|0456787283|FX1 Small Adult A...| null|[WrappedArray(Clo...|\n",
      "|0456838384|Riz Small Unisex ...| null|[WrappedArray(Clo...|\n",
      "+----------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset2 : Amazon Product information\n",
    "# First, define Schema for second Dataset\n",
    "PRODUCTS_SCHEMA_DEF = StructType([\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('title', StringType(), True),\n",
    "        StructField('price', DoubleType(), True),\n",
    "        StructField('categories', ArrayType(ArrayType(\n",
    "            StringType(), True),True),True)\n",
    "    ])\n",
    "\n",
    "# Load the dataset\n",
    "productDF = sqlContext.read.json(product_filepaths,PRODUCTS_SCHEMA_DEF)\n",
    "productDF.show()\n",
    "# productDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25566"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrichedReviews = filteredDF.join(productDF, productDF.asin==filteredDF.asin).dropna(subset=\"title\")\n",
    "enrichedReviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you join two RDDs, you have to restructure the data into (k,V) pairs where the key is the join key. This may involve two additional map transformations. This is not necessary in DataFrames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, overall: double, reviewText: string, helpful: double, reviewerID: string, unixReviewTime: bigint, asin: string, title: string, price: double, categories: array<array<string>>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrichedReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------------+--------------+--------------+----------+--------------------+------+--------------------+\n",
      "|      asin|overall|          reviewText|           helpful|    reviewerID|unixReviewTime|      asin|               title| price|          categories|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+----------+--------------------+------+--------------------+\n",
      "|0132793040|    5.0|Corey Barker does...|               1.0| AKM1MP6P0OYPR|    1365811200|0132793040|Kelby Training DV...|  null|[WrappedArray(Ele...|\n",
      "|0321732944|    5.0|While many beginn...|               0.0|A2CX7LUOHB2NDG|    1341100800|0321732944|Kelby Training DV...|  null|[WrappedArray(Ele...|\n",
      "|0439886341|    3.0|Some of the funct...|               1.0|A2WNBOD3WNDNKT|    1374451200|0439886341|Digital Organizer...|  8.15|[WrappedArray(Ele...|\n",
      "|0511189877|    5.0|Dog got the old r...|               0.0|A1QGNMC6O1VW39|    1397433600|0511189877|CLIKR-5 Time Warn...| 23.36|[WrappedArray(Ele...|\n",
      "|0511189877|    5.0|We had an old Tim...|               0.0|A2TY0BTJOTENPG|    1395878400|0511189877|CLIKR-5 Time Warn...| 23.36|[WrappedArray(Ele...|\n",
      "|0511189877|    5.0|This unit works j...|               0.0|A34ATBPOK6HCHY|    1395532800|0511189877|CLIKR-5 Time Warn...| 23.36|[WrappedArray(Ele...|\n",
      "|0511189877|    5.0|It is an exact du...|               0.0| A89DO69P0XZ27|    1395446400|0511189877|CLIKR-5 Time Warn...| 23.36|[WrappedArray(Ele...|\n",
      "|0511189877|    5.0|Works on my t.v. ...|               0.0| AZYNQZ94U6VDB|    1401321600|0511189877|CLIKR-5 Time Warn...| 23.36|[WrappedArray(Ele...|\n",
      "|0528881469|    5.0|Love it has every...|               0.0|A1DA3W4GTFXP6O|    1405641600|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    5.0|We got this GPS f...|               0.0| AO94DHGC771SJ|    1370131200|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    4.0|This is a great t...|0.9545454545454546|A28B1G1MSJ6OO1|    1280016000|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    3.0|Well, what can I ...|0.9555555555555556|A3N7T0DY83Y4IG|    1283990400|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    4.0|This unit is a fa...|               1.0|A265MKAR2WEH3Y|    1294790400|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    5.0|I did a lot of co...|               1.0|A37K02NKUIT68K|    1293235200|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    4.0|I purchased this ...|               0.5|A2AW1SSVUIYV9Y|    1289001600|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|0.7142857142857143|A2AEHUKOV014BP|    1284249600|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    4.0|Well as one of th...|               1.0|A2O8FIJR9EBU56|    1278547200|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    4.0|Was fast and what...|               0.0| AYTBGUX49LF3W|    1398470400|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    5.0|We had the GPS fo...|               0.0|A1E4WG8HRWWK4R|    1390867200|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "|0528881469|    5.0|Back in the old d...|               0.5|A2AOEW5UGXFOOQ|    1294790400|0528881469|Rand McNally 5288...|299.99|[WrappedArray(Ele...|\n",
      "+----------+-------+--------------------+------------------+--------------+--------------+----------+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enrichedReviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7. Saving your DataFrame</h1> \n",
    "\n",
    "Now that we have done some operations on the data, we can save the file for later use. Standard data formats are a great way to opening up valuable data to your entire organization. Spark DataFrames can be saved in many different formats including and not limited to JSON, parquet, Hive and etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR !!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    columnDF.write.parquet('Data/Outputs/reviews_filtered.parquet')\n",
    "    print \"Saved as parquet successfully\"\n",
    "except:\n",
    "    print \"ERROR !!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>8. Using Spark SQL</h1>\n",
    "\n",
    "Spark DataFrames also allow you to use Spark SQL to query from Petabytes of data. Spark comes with a SQL like query language which can be used to query from Distributed DataFrames. A key advantage of using Spark SQL is that the [Catelyst query optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Catelyst\") under the hood transforms your SQL query to run it most efficiently. \n",
    "\n",
    "<h2>8.1. Example Queries</h2>\n",
    "\n",
    "Spark SQL can leverage the same functionality as the DataFrame API provides. In fact, it provides more functionality via SQL capabilities and HQL capabilities that are available to Spark SQL environment. \n",
    "\n",
    "For the sake of time constrains, I will explain different functions available in Spark SQL environment by using examples that use multiple functions. This will benefit by:\n",
    "+ Covering many functions that are possible via spark SQL\n",
    "+ Giving an understanding about how to pipe multiple functions together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30000 reviews about 2469 products\n"
     ]
    }
   ],
   "source": [
    "# Read the reviews parquet file\n",
    "reviewsDF = sqlContext.read.parquet('Data/Outputs/reviews_filtered.parquet')\n",
    "\n",
    "# Register the DataFrames to be used in sql\n",
    "reviewsDF.registerTempTable(\"reviews\")\n",
    "productDF.registerTempTable(\"products\")\n",
    "\n",
    "print 'There are {0} reviews about {1} products'.format(reviewsDF.count(),productDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------+\n",
      "|      asin|overall|          reviewText| price|\n",
      "+----------+-------+--------------------+------+\n",
      "|0528881469|    5.0|Love it has every...|299.99|\n",
      "|0528881469|    1.0|I have owned two ...|299.99|\n",
      "|0528881469|    5.0|We got this GPS f...|299.99|\n",
      "|0528881469|    1.0|I'm a professiona...|299.99|\n",
      "|0528881469|    4.0|This is a great t...|299.99|\n",
      "|0528881469|    3.0|Well, what can I ...|299.99|\n",
      "|0528881469|    2.0|Not going to writ...|299.99|\n",
      "|0528881469|    2.0|My brother is a t...|299.99|\n",
      "|0528881469|    4.0|This unit is a fa...|299.99|\n",
      "|0528881469|    5.0|I did a lot of co...|299.99|\n",
      "|0528881469|    4.0|I purchased this ...|299.99|\n",
      "|0528881469|    5.0|EXCELLENT. BEST T...|299.99|\n",
      "|0528881469|    1.0|I was real psyche...|299.99|\n",
      "|0528881469|    4.0|Well as one of th...|299.99|\n",
      "|0528881469|    1.0|Thought the unit ...|299.99|\n",
      "|0528881469|    4.0|Was fast and what...|299.99|\n",
      "|0528881469|    2.0|Twice this item h...|299.99|\n",
      "|0528881469|    1.0|DONT WAIST YOUR M...|299.99|\n",
      "|0528881469|    5.0|We had the GPS fo...|299.99|\n",
      "|0528881469|    5.0|Back in the old d...|299.99|\n",
      "+----------+-------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT reviews.asin, overall, reviewText, price\n",
    "            FROM reviews JOIN products ON  reviews.asin=products.asin\n",
    "            WHERE price > 50.00\n",
    "\"\"\"\n",
    "\n",
    "result = sqlContext.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional: User Defined Functions</h1>\n",
    "\n",
    "Spark SQL also provides the functionality similar to User Defined Functions (UDF) offering in Hive. Spark uses registerFunction() function to register python functions in SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve 'cleaned' given input columns asin, overall, reviewText, price;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-71568521d22f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mresult_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mariamestre/Documents/spark/spark-1.6.0/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mariamestre/Documents/spark/spark-1.6.0/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mariamestre/Documents/spark/spark-1.6.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve 'cleaned' given input columns asin, overall, reviewText, price;\""
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def transform_review(review):\n",
    "    x1 = re.sub('[^0-9a-zA-Z\\s]+','',review)\n",
    "    return x1.lower()\n",
    "\n",
    "result.registerTempTable(\"result\")\n",
    "sqlContext.registerFunction(\"to_lowercase\", lambda x:transform_review(x), returnType=StringType())\n",
    "\n",
    "sql_query_transform = \"\"\"SELECT asin, reviewText, to_lowercase(reviewText) as cleaned\n",
    "            FROM result\n",
    "\"\"\"\n",
    "\n",
    "result_transform = sqlContext.sql(sql_query_transform)\n",
    "result_transform.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional : Mix and Match!!</h1>\n",
    "\n",
    "You can also mix DataFrames, RDDs and SparkSQL to make it work for you. \n",
    "\n",
    "<h2>Scenario</h2>\n",
    "\n",
    "We want to investigate the average rating of reviews in terms of the categories they belong to. In order to do this, we:\n",
    "+ query the needed data using DataFrames API\n",
    "+ classify the reviews into different categories using core RDD API\n",
    "+ query the avearage rating for each category using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+\n",
      "|      asin|              review|   category|\n",
      "+----------+--------------------+-----------+\n",
      "|0528881469|Love it has every...|electronics|\n",
      "|0528881469|I have owned two ...|electronics|\n",
      "|0528881469|We got this GPS f...|electronics|\n",
      "|0528881469|I'm a professiona...|electronics|\n",
      "|0528881469|This is a great t...|electronics|\n",
      "|0528881469|Well, what can I ...|electronics|\n",
      "|0528881469|Not going to writ...|electronics|\n",
      "|0528881469|My brother is a t...|electronics|\n",
      "|0528881469|This unit is a fa...|     sports|\n",
      "|0528881469|I did a lot of co...|electronics|\n",
      "|0528881469|I purchased this ...|electronics|\n",
      "|0528881469|EXCELLENT. BEST T...|electronics|\n",
      "|0528881469|I was real psyche...|electronics|\n",
      "|0528881469|Well as one of th...|electronics|\n",
      "|0528881469|Thought the unit ...|electronics|\n",
      "|0528881469|Was fast and what...|     sports|\n",
      "|0528881469|Twice this item h...|     sports|\n",
      "|0528881469|DONT WAIST YOUR M...|electronics|\n",
      "|0528881469|We had the GPS fo...|electronics|\n",
      "|0528881469|Back in the old d...|electronics|\n",
      "+----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import cPickle\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "model = cPickle.load(open('Data/classifiers/classifier.pkl', 'r'))\n",
    "classifier_b = sc.broadcast(model)\n",
    "\n",
    "classifiedRDD = result_transform.filter(\"cleaned <> ''\")\\\n",
    "                                .map(lambda row: \n",
    "                                     (row.asin,row.reviewText,str(classifier_b.value.predict([row.reviewText])[0]))\n",
    "                                    )\n",
    "\n",
    "CLASSIFIED_SCHEMA = StructType([\n",
    "        StructField('asin', StringType(), True),\n",
    "        StructField('review', StringType(), True),\n",
    "        StructField('category', StringType(), True)\n",
    "    ])\n",
    "\n",
    "classifiedDF = sqlContext.createDataFrame(classifiedRDD,CLASSIFIED_SCHEMA)\n",
    "\n",
    "classifiedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|   category|        avgRating|\n",
      "+-----------+-----------------+\n",
      "|electronics|3.767588990706229|\n",
      "|    fashion| 4.00352086576236|\n",
      "|     sports|4.287197603795619|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiedDF.registerTempTable('enrichedReviews')\n",
    "\n",
    "sql_query_test = \"\"\"SELECT category, avg(overall) as avgRating\n",
    "            FROM reviews \n",
    "            JOIN products ON reviews.asin=products.asin \n",
    "            JOIN enrichedReviews ON products.asin=enrichedReviews.asin\n",
    "            WHERE price > 50.0\n",
    "            GROUP BY enrichedReviews.category\n",
    "\"\"\"\n",
    "\n",
    "resultTest = sqlContext.sql(sql_query_test)\n",
    "resultTest.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
